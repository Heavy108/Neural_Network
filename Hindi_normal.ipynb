{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makemore but with MLP\n",
    "\n",
    "based on this paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"hindi.txt\", \"r\").read().split()\n",
    "words = [\n",
    "    w\n",
    "    for w in words\n",
    "    # if \"‡ßß\"\n",
    "    # and \"‡ß®\"\n",
    "    # and \"‡ß©\"\n",
    "    # and \"‡ß™\"\n",
    "    # and \"‡ß´\"\n",
    "    # and \"‡ß¨\"\n",
    "    # and \"‡ß≠\"\n",
    "    # and \"‡ßÆ\"\n",
    "    # and \"‡ßØ\"\n",
    "    # and \"‡ß¶\" not in w\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list((set(\"\".join(words)))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(characters)}\n",
    "stoi[\"~\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(characters) + 1\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "\n",
    "block_size = 3  # context length: how many characters do we need to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words[:2]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + \"~\":\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(\"\".join(itos[i] for i in context), \"-->\", itos[ix])\n",
    "        context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=vocab).float() @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Net ü•Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "\n",
    "block_size = 3  # context length: how many characters do we need to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + \"~\":\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 2))\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, vocab))\n",
    "b2 = torch.randn(vocab)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "losses = []\n",
    "iters = 20\n",
    "for _ in range(iters):\n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    losses.append(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "\n",
    "plt.plot(range(iters), losses)\n",
    "plt.title(\"Change in loss with each iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "print(f\"Final Loss = {losses.pop()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this took up way too much time. so let's do this in batches, or rather ‚ú®*mini-batches*‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, X.size(0), (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 2))\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, vocab))\n",
    "b2 = torch.randn(vocab)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 10000\n",
    "losses = []\n",
    "for _ in range(iters):\n",
    "    # mini batch\n",
    "    ix = torch.randint(0, X.size(0), (200,))\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # losses.append(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.013 * p.grad\n",
    "\n",
    "\n",
    "# plt.plot(range(iters), losses)\n",
    "# plt.title(\"Change in loss with each iteration (for the mini-batch)\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"iterations\")\n",
    "# print(f\"Final Loss = {losses.pop()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well, but how do you find the optimal learning rate?\n",
    "\n",
    "We experiment a bit to find out the range in which the rate is not cringe. Then we start from the big number of that range and slide into the lower bound with each iteration. We then finally choose the learning rate that gives the best result.\n",
    "\n",
    "![](https://media.tenor.com/A6yz-HeOGIgAAAAM/leonardo-dicaprio-leonardo-dicaprio-meme.gif)\n",
    "\n",
    "### Learning rate decay\n",
    "\n",
    "We often lower the learning rate in the later stages of learning. This is called learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 2))\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, vocab))\n",
    "b2 = torch.randn(vocab)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "losses = []\n",
    "lrs = torch.linspace(0.01, 0.15, 1000)\n",
    "for i in range(iters):\n",
    "    # mini batch\n",
    "    ix = torch.randint(0, X.size(0), (200,))\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    losses.append(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -lrs[i] * p.grad\n",
    "\n",
    "\n",
    "plt.plot(lrs.tolist(), losses)\n",
    "plt.title(\"Change in loss with learing rate\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"learing rate\")\n",
    "plt.grid()\n",
    "print(f\"Final Loss = {losses.pop()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get real\n",
    "\n",
    "It shoud come at no surprise that this not how we do shit in prod. Irl, we have several splits/phases for preparing our neural net for the real world.\n",
    "\n",
    "| ‚û°Ô∏è | training split | dev/validation split | test split |\n",
    "|--------|----------------|----------------------|------------|\n",
    "| shares | 80%            | 10%                  | 10%        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 8\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \"~\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(f\"{X.shape=}, {Y.shape=}\")\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.seed(2002)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 2))\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, vocab))\n",
    "b2 = torch.randn(vocab)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = None\n",
    "for i in range(40000):\n",
    "    # mini batch\n",
    "    ix = torch.randint(0, Xtr.size(0), (200,))\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of the neural net on our dev split. Keep in mind that these are the data that the neural net has not seen and is not trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the loss\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is the loss for the training set itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the loss\n",
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set and dev set losses are not equal. This means the model is not overfitted. That implies the model is not 'memorizing' the training data.\n",
    "\n",
    "But, since the training and dev dataset losses are roughly equal, this means that the model is underfitted. This is a good hint that our network is very tiny and we need to expand our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 2))\n",
    "W1 = torch.randn((6, 300))\n",
    "b1 = torch.randn(300)\n",
    "W2 = torch.randn((300, vocab))\n",
    "b2 = torch.randn(vocab)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for i in range(40000):\n",
    "    # mini batch\n",
    "    ix = torch.randint(0, Xtr.size(0), (32,))\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.01 * p.grad\n",
    "    steps.append(i)\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the loss\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the loss\n",
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(C[:, 0].data.tolist(), C[:, 1].data.tolist(), s=200, color=\"green\")\n",
    "for i in range(C.size(0)):\n",
    "    plt.text(\n",
    "        C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\"\n",
    "    )\n",
    "plt.grid(\"minor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we observe we can't go much beyond our previous simple neural net, and we have already played with the weights, biases and tried expanding the number of neurons, we are begininning to suspect that the dimension of the embedding vector might be the bottleneck.\n",
    "\n",
    "So, next we try changing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab, 20))\n",
    "W1 = torch.randn((60, 300)) * 0.1\n",
    "b1 = torch.randn(300) * 0.01\n",
    "W2 = torch.randn((300, vocab)) * 0.1\n",
    "b2 = torch.randn(vocab) * 0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 50000\n",
    "for i in range(max_steps):\n",
    "    # mini batch\n",
    "    ix = torch.randint(0, Xtr.size(0), (32,))\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 60) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.01 * p.grad\n",
    "    steps.append(i)\n",
    "    losses.append(loss.log10().item())\n",
    "    # track stats\n",
    "    if i % 10000 == 0:  # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the loss\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 60) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the loss\n",
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, 60) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "# g = torch.Generator().manual_seed(2002)\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]  # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample from the model\n",
    "# # g = torch.Generator().manual_seed(2002)\n",
    "# for _ in range(10):\n",
    "#     out = []\n",
    "#     context = [stoi[i] for i in \"‡§™‡•ç‡§∞‡§¶‡•á‡§∂\"]  # initialize with all ...\n",
    "#     while True:\n",
    "#         emb = C[torch.tensor([context])]  # (1,block_size,d)\n",
    "#         h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "#         logits = h @ W2 + b2\n",
    "#         # probs = F.softmax(logits, dim=1)\n",
    "#         ix = torch.multinomial(probs, num_samples=1).item()\n",
    "#         context = context[1:] + [ix]\n",
    "#         out.append(ix)\n",
    "#         if ix == 0:\n",
    "#             break\n",
    "#     print(\"\".join(itos[i] for i in out), end=\" \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
